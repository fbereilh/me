{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "time_run": "2026-02-12T22:28:47.210898+00:00"
   },
   "source": [
    "---\n",
    "title: \"Building a Smart Scheduling Agent with LangChain\"\n",
    "author: \"Felipe Bereilh\"\n",
    "date: \"2026-02-10\"\n",
    "categories: [agents]\n",
    "description: \"My todo list mocks me. It grows faster than I can check things off\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "time_run": "2026-02-12T21:16:03.735401+00:00"
   },
   "source": [
    "# Building a Smart Scheduling Agent with LangChain\n",
    "\n",
    "> My todo list mocks me. It grows faster than I can check things off\n",
    "\n",
    "---\n",
    "\n",
    "I'm terrible at estimating how long things take. That \"quick 30-minute task\" somehow devours two hours. And by the time I realize it, my carefully planned day is in ruins.\n",
    "\n",
    "So I decided to build an AI agent to do what I can't: estimate task durations realistically, look at my actual calendar, and schedule everything so it actually fits. Better yet â€” it learns from my mistakes.\n",
    "\n",
    "In this post, I'll walk through building a productivity agent with LangChain. Along the way, we'll explore how LangChain's tool-calling works under the hood, and create something genuinely useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "time_run": "2026-02-12T22:26:40.073943+00:00"
   },
   "source": [
    "## The plan\n",
    "\n",
    "\n",
    "**Core Features:**\n",
    "1. **Time estimation** â€“ LLM infers how long each task takes  \n",
    "2. **Calendar integration** â€“ reads your available time slots  \n",
    "3. **Smart scheduling** â€“ fits tasks into slots, splitting if needed  \n",
    "4. **Priority management** â€“ respects your priorities but suggests swaps  \n",
    "5. **Learning loop** â€“ tracks actual vs estimated time to improve predictions  \n",
    "\n",
    "**Architecture we'll need:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   UI/Input  â”‚â”€â”€â”€â”€â–¶â”‚  LLM Agent  â”‚â”€â”€â”€â”€â–¶â”‚  Calendar   â”‚\n",
    "â”‚   (todos)   â”‚     â”‚  (LangChain)â”‚     â”‚    API      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Storage  â”‚\n",
    "                    â”‚(estimates,â”‚\n",
    "                    â”‚  actuals) â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "time_run": "2026-02-12T21:29:25.130838+00:00"
   },
   "source": [
    "## What is LangChain?\n",
    "\n",
    "LangChain is a framework for building applications with LLMs. Its key concepts are:\n",
    "\n",
    "1. **Models** â€“ wrappers around LLMs (OpenAI, Anthropic, etc.)\n",
    "2. **Tools** â€“ functions the LLM can call (like checking your calendar)\n",
    "3. **Agents** â€“ LLMs that decide *which* tools to use and *when*\n",
    "4. **Chains** â€“ sequences of steps piped together\n",
    "\n",
    "For our scheduling agent, we'll mainly use **Tools** and **Agents** â€” the LLM will decide when to estimate time, check the calendar, or schedule a task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "time_run": "2026-02-12T21:26:48.635144+00:00"
   },
   "source": [
    "## So, what Even Is a Tool?\n",
    "\n",
    "In LangChain-land, a \"tool\" is just a Python function that an LLM can ask to use. That's it. No magic.\n",
    "\n",
    "You create one with the `@tool` decorator:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "time_run": "2026-02-12T22:29:20.766358+00:00"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def estimate_duration(task_description: str) -> int:\n",
    "    \"\"\"Estimate how many minutes a task will take.\"\"\"\n",
    "    return 30  # placeholder for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "time_run": "2026-02-12T22:29:21.323500+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate_duration\n",
      "Estimate how many minutes a task will take.\n"
     ]
    }
   ],
   "source": [
    "print(estimate_duration.name)\n",
    "print(estimate_duration.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "time_run": "2026-02-12T21:27:49.005489+00:00"
   },
   "source": [
    "The LLM sees the function name and docstring â€” that's how it decides *when* to use it. Think of the docstring as the tool's sales pitch.\n",
    "\n",
    "## The Two-Step Dance\n",
    "\n",
    "Here's the part that confused me at first: **the LLM never actually runs your code.**\n",
    "\n",
    "When you ask \"How long will writing a blog post take?\", the LLM responds with something like:\n",
    "\n",
    "> \"I'd like to call `estimate_duration` with `'write a blog post'`\"\n",
    "\n",
    "That's it. It's a *request*, not an execution. Your code has to actually run the function and feed the result back. It's a polite back-and-forth â€” the LLM proposes, you execute.\n",
    "\n",
    "This is where agents come in, but we'll get there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "time_run": "2026-02-12T21:35:51.943543+00:00"
   },
   "source": [
    "\n",
    "## Setting Up the Model\n",
    "\n",
    "First, let's connect to an LLM. LangChain supports many providers â€” we'll use Anthropic's Claude.\n",
    "\n",
    "You'll need an API key from [Anthropic's console](https://console.anthropic.com/). Set it as an environment variable:\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=\"your-key-here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "time_run": "2026-02-12T22:30:53.134227+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there, how are you?\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-sonnet-4-20250514\")\n",
    "\n",
    "# Quick sanity check to make sure it's working:\n",
    "response = llm.invoke(\"Say hello in exactly 5 words\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "time_run": "2026-02-12T21:36:14.031062+00:00"
   },
   "source": [
    "We're in business. Now let's give this LLM some superpowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "time_run": "2026-02-12T22:32:03.856463+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'estimate_duration', 'args': {'task_description': 'write a blog post'}, 'id': 'toolu_018Ekrhrpq3eLwzw3FRhHDnu', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools([estimate_duration])\n",
    "response = llm_with_tools.invoke(\"How long will it take to write a blog post?\")\n",
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "time_run": "2026-02-12T21:41:40.936942+00:00"
   },
   "source": [
    "What just happened?\n",
    "\n",
    "**The LLM didn't actually run your function.** Instead, it said: *\"I want to call `estimate_duration` with this argument.\"*\n",
    "\n",
    "Look at what came back:\n",
    "```python\n",
    "{\n",
    "    'name': 'estimate_duration',           # which tool to call\n",
    "    'args': {'task_description': 'write a blog post'},  # with what input\n",
    "    'id': 'toolu_015...',                  # unique ID for this call\n",
    "    'type': 'tool_call'\n",
    "}\n",
    "```\n",
    "\n",
    "This is the key insight: **tool calling is a two-step dance**:\n",
    "\n",
    "1. **LLM decides** â€” \"I should use this tool with these arguments\"\n",
    "2. **Your code executes** â€” you actually run the function and feed the result back\n",
    "\n",
    "The LLM is essentially saying *\"please call this function for me\"* â€” it can't run Python itself!\n",
    "\n",
    "---\n",
    "\n",
    "This is where **Agents** come in. An agent automates this loop: ask LLM â†’ run tool â†’ feed result back â†’ repeat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "solveit_ai": true,
    "time_run": "2026-02-12T22:28:13.899693+00:00"
   },
   "source": [
    "show me the agent creation in lang chain\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "Here's how to create an agent that handles the tool-calling loop automatically:\n",
    "\n",
    "```python\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Combine your tools\n",
    "tools = [estimate_duration]  # add more tools here as you build them\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Now you can invoke it and it handles the full loop\n",
    "result = agent.invoke({\"messages\": [(\"user\", \"How long will writing a blog post take?\")]})\n",
    "print(result[\"messages\"][-1].content)\n",
    "```\n",
    "\n",
    "The `create_react_agent` function creates a \"ReAct\" agent â€” it **Re**asons about what to do, then **Act**s by calling tools, then observes the result, and repeats until it has an answer.\n",
    "\n",
    "Try adding this to your post and running it. Does the output make sense? Then we can move on to adding the calendar tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "time_run": "2026-02-12T22:33:05.778231+00:00"
   },
   "source": [
    "## Enter the Agent\n",
    "Remember that two-step dance? Ask the LLM, run the function, feed the result back? Yeah, I don't want to do that manually every time. That's exactly the kind of tedious loop computers were invented for.\n",
    "\n",
    "LangChain's `create_react_agent` handles this for us. \"ReAct\" stands for Reason + Act â€” the agent thinks about what to do, calls a tool, observes the result, and keeps going until it has an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "time_run": "2026-02-12T22:29:36.858170+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_575/1377822623.py:7: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm, tools)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the estimate, writing a blog post will take approximately **30 minutes**.\n",
      "\n",
      "Keep in mind that this is a general estimate and the actual time can vary significantly depending on factors like:\n",
      "- The length and complexity of the blog post\n",
      "- How much research is needed\n",
      "- Your writing experience and speed\n",
      "- Whether you need to include images, links, or other media\n",
      "- The amount of editing and proofreading required\n",
      "\n",
      "A simple, short blog post might take less time, while a comprehensive, well-researched article could take several hours.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Combine your tools\n",
    "tools = [estimate_duration]  # add more tools here as you build them\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Now you can invoke it and it handles the full loop\n",
    "result = agent.invoke({\"messages\": [(\"user\", \"How long will writing a blog post take?\")]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "time_run": "2026-02-12T22:34:09.356804+00:00"
   },
   "source": [
    "That's it. No manual back-and-forth. The agent sees my question, realizes it needs the estimate_duration tool, calls it, gets \"30 minutes,\" and wraps everything into a friendly response.\n",
    "\n",
    "It's like having a very polite assistant who knows when to check their reference materials instead of just making stuff up."
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
