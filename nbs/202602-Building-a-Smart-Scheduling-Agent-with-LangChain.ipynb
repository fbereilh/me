{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "time_run": "2026-02-12T22:28:47.210898+00:00"
   },
   "source": [
    "---\n",
    "title: \"Building a Smart Scheduling Agent with LangChain\"\n",
    "author: \"Felipe Bereilh\"\n",
    "date: \"2026-02-10\"\n",
    "categories: [agents]\n",
    "description: \"My todo list mocks me. It grows faster than I can check things off\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "time_run": "2026-02-12T21:16:03.735401+00:00"
   },
   "source": [
    "# Building a Smart Scheduling Agent with LangChain\n",
    "\n",
    "> My todo list mocks me. It grows faster than I can check things off\n",
    "\n",
    "---\n",
    "\n",
    "I'm terrible at estimating how long things take. That \"quick 30-minute task\" somehow devours two hours. And by the time I realize it, my carefully planned day is in ruins.\n",
    "\n",
    "So I decided to build an AI agent to do what I can't: estimate task durations realistically, look at my actual calendar, and schedule everything so it actually fits. Better yet — it learns from my mistakes.\n",
    "\n",
    "In this post, I'll walk through building a productivity agent with LangChain. Along the way, we'll explore how LangChain's tool-calling works under the hood, and create something genuinely useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "time_run": "2026-02-12T22:39:05.929023+00:00"
   },
   "source": [
    "## The plan\n",
    "\n",
    "\n",
    "**Core Features:**  \n",
    "1. **Time estimation** – LLM infers how long each task takes  \n",
    "2. **Calendar integration** – reads your available time slots  \n",
    "3. **Smart scheduling** – fits tasks into slots, splitting if needed  \n",
    "4. **Priority management** – respects your priorities but suggests swaps  \n",
    "5. **Learning loop** – tracks actual vs estimated time to improve predictions  \n",
    "\n",
    "**Architecture we'll need:**\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
    "│   UI/Input  │────▶│  LLM Agent  │────▶│  Calendar   │\n",
    "│   (todos)   │     │  (LangChain)│     │    API      │\n",
    "└─────────────┘     └──────┬──────┘     └─────────────┘\n",
    "                          │\n",
    "                    ┌─────▼─────┐\n",
    "                    │  Storage  │\n",
    "                    │(estimates,│\n",
    "                    │  actuals) │\n",
    "                    └───────────┘\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "time_run": "2026-02-12T21:29:25.130838+00:00"
   },
   "source": [
    "## What is LangChain?\n",
    "\n",
    "LangChain is a framework for building applications with LLMs. Its key concepts are:\n",
    "\n",
    "1. **Models** – wrappers around LLMs (OpenAI, Anthropic, etc.)\n",
    "2. **Tools** – functions the LLM can call (like checking your calendar)\n",
    "3. **Agents** – LLMs that decide *which* tools to use and *when*\n",
    "4. **Chains** – sequences of steps piped together\n",
    "\n",
    "For our scheduling agent, we'll mainly use **Tools** and **Agents** — the LLM will decide when to estimate time, check the calendar, or schedule a task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "time_run": "2026-02-12T21:26:48.635144+00:00"
   },
   "source": [
    "## So, what Even Is a Tool?\n",
    "\n",
    "In LangChain-land, a \"tool\" is just a Python function that an LLM can ask to use. That's it. No magic.\n",
    "\n",
    "You create one with the `@tool` decorator:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "time_run": "2026-02-12T22:42:10.041255+00:00"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def estimate_duration(task_description: str) -> int:\n",
    "    \"\"\"Estimate how many minutes a task will take.\"\"\"\n",
    "    return 30  # placeholder for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "time_run": "2026-02-12T22:42:10.069741+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate_duration\n",
      "Estimate how many minutes a task will take.\n"
     ]
    }
   ],
   "source": [
    "print(estimate_duration.name)\n",
    "print(estimate_duration.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "time_run": "2026-02-12T21:27:49.005489+00:00"
   },
   "source": [
    "The LLM sees the function name and docstring — that's how it decides *when* to use it. Think of the docstring as the tool's sales pitch.\n",
    "\n",
    "## The Two-Step Dance\n",
    "\n",
    "Here's the part that confused me at first: **the LLM never actually runs your code.**\n",
    "\n",
    "When you ask \"How long will writing a blog post take?\", the LLM responds with something like:\n",
    "\n",
    "> \"I'd like to call `estimate_duration` with `'write a blog post'`\"\n",
    "\n",
    "That's it. It's a *request*, not an execution. Your code has to actually run the function and feed the result back. It's a polite back-and-forth — the LLM proposes, you execute.\n",
    "\n",
    "This is where agents come in, but we'll get there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "time_run": "2026-02-12T21:35:51.943543+00:00"
   },
   "source": [
    "\n",
    "## Setting Up the Model\n",
    "\n",
    "First, let's connect to an LLM. LangChain supports many providers — we'll use Anthropic's Claude.\n",
    "\n",
    "You'll need an API key from [Anthropic's console](https://console.anthropic.com/). Set it as an environment variable:\n",
    "\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=\"your-key-here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "time_run": "2026-02-12T22:42:10.093503+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there, how are you?\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-sonnet-4-20250514\")\n",
    "\n",
    "# Quick sanity check to make sure it's working:\n",
    "response = llm.invoke(\"Say hello in exactly 5 words\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "time_run": "2026-02-12T21:36:14.031062+00:00"
   },
   "source": [
    "We're in business. Now let's give this LLM some superpowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "time_run": "2026-02-12T22:42:11.941911+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'estimate_duration', 'args': {'task_description': 'write a blog post'}, 'id': 'toolu_01VzcaBjFXpStLFXhBAgp87h', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools([estimate_duration])\n",
    "response = llm_with_tools.invoke(\"How long will it take to write a blog post?\")\n",
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "time_run": "2026-02-12T22:42:42.054913+00:00"
   },
   "source": [
    "What just happened?\n",
    "\n",
    "**The LLM didn't actually run your function.** Instead, it said: *\"I want to call `estimate_duration` with this argument.\"*\n",
    "\n",
    "Look at what came back:\n",
    "```python\n",
    "{\n",
    "    'name': 'estimate_duration',           # which tool to call\n",
    "    'args': {'task_description': 'write a blog post'},  # with what input\n",
    "    'id': 'toolu_015...',                  # unique ID for this call\n",
    "    'type': 'tool_call'\n",
    "}\n",
    "```\n",
    "\n",
    "This is the key insight: **tool calling is a two-step dance**:\n",
    "\n",
    "1. **LLM decides** — \"I should use this tool with these arguments\"\n",
    "2. **Your code executes** — you actually run the function and feed the result back\n",
    "\n",
    "The LLM is essentially saying *\"please call this function for me\"* — it can't run Python itself!  \n",
    "This is where **Agents** come in. An agent automates this loop: ask LLM → run tool → feed result back → repeat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "time_run": "2026-02-12T22:33:05.778231+00:00"
   },
   "source": [
    "## Enter the Agent\n",
    "Remember that two-step dance? Ask the LLM, run the function, feed the result back? Yeah, I don't want to do that manually every time. That's exactly the kind of tedious loop computers were invented for.\n",
    "\n",
    "LangChain's `create_react_agent` handles this for us. \"ReAct\" stands for Reason + Act — the agent thinks about what to do, calls a tool, observes the result, and keeps going until it has an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "time_run": "2026-02-12T22:42:13.538085+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_575/3338827541.py:8: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm, tools)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the estimation, writing a blog post will take approximately **30 minutes**.\n",
      "\n",
      "However, this is a general estimate and the actual time can vary significantly depending on factors such as:\n",
      "- The length and complexity of the blog post\n",
      "- Your writing experience and speed\n",
      "- Whether you need to do research\n",
      "- The amount of editing and revision required\n",
      "- Whether you're including images, links, or other media\n",
      "\n",
      "A simple, short blog post might take less time, while a detailed, well-researched article could take several hours.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Combine your tools\n",
    "tools = [estimate_duration]  # add more tools here as you build them\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "\n",
    "# Now you can invoke it and it handles the full loop\n",
    "result = agent.invoke({\"messages\": [(\"user\", \"How long will writing a blog post take?\")]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "time_run": "2026-02-12T22:34:09.356804+00:00"
   },
   "source": [
    "That's it. No manual back-and-forth. The agent sees my question, realizes it needs the estimate_duration tool, calls it, gets \"30 minutes,\" and wraps everything into a friendly response.\n",
    "\n",
    "It's like having a very polite assistant who knows when to check their reference materials instead of just making stuff up."
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
